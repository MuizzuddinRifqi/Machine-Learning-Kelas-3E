{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nwvw6hyqUzrc"
      },
      "source": [
        "### Import TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ys1pI_c5Uzrg"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0Iw2YF5Uzrh"
      },
      "source": [
        "### Download Dataset Shakespeare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDI4cjqiUzri",
        "outputId": "37411fa6-af27-4ab7-9572-417f6f3c9b84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buQOnNW4Uzri"
      },
      "source": [
        "Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9sZ5ctAUzrj",
        "outputId": "34f17f7a-d577-4b7c-c03c-7d5d1bf60c76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ],
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChFNUvLLUzrj",
        "outputId": "9aec4993-a4c2-4762-d313-f8fb7e2fdcf6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HZY-reXUzrj",
        "outputId": "9586ec39-8c43-4185-a831-00f052652aa6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "65 unique characters\n"
          ]
        }
      ],
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcUJRaXFUzrj"
      },
      "source": [
        "### Olah Teks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l15iwtzOUzrj"
      },
      "source": [
        "Vectorize Teks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IU1iij2CUzrj",
        "outputId": "50754308-2130-4c5c-d23f-b26fb120f60a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LzEh4gVVUzrj"
      },
      "outputs": [],
      "source": [
        "\n",
        "ids_from_chars = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCYOze7gUzrk",
        "outputId": "758f8870-020e-4a4a-a5f1-127774a0eb20"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "sqCxDZ0rUzrk"
      },
      "outputs": [],
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRZmW84UUzrk",
        "outputId": "e6a1c821-75c5-4ff1-ca3f-e86f80ad3c05"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0aCGGmOUzrk",
        "outputId": "eed7bacb-1cdf-48f5-d02a-3040a71d4b3a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "19mABPfEUzrk"
      },
      "outputs": [],
      "source": [
        "def text_from_ids(ids):\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zcCMbS2Uzrk"
      },
      "source": [
        "### Membuat Trianing Set dan Target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xmxvtMiUzrk",
        "outputId": "ed8ace55-ab03-4654-cbad-92ae406b037b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Uglw_a8aUzrl"
      },
      "outputs": [],
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9MMpBIXUzrl",
        "outputId": "d575da10-0a3b-4b1d-8874-035e037ac058"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ],
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "jTy63gBlUzrl"
      },
      "outputs": [],
      "source": [
        "seq_length = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MY3dBm4IUzrl",
        "outputId": "0466c602-afaa-4027-b320-d15fbf3d8320"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9OXANDLUzrl",
        "outputId": "9ac52c1c-6158-4fa7-8046-dc8353d2cb9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ],
      "source": [
        "for seq in sequences.take(5):\n",
        "    print(text_from_ids(seq).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "FoQ5A9h5Uzrl"
      },
      "outputs": [],
      "source": [
        "def split_input_target(sequence):\n",
        "  input_text = sequence[:-1]\n",
        "  target_text = sequence[1:]\n",
        "  return input_text, target_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeUCl8TeUzrl",
        "outputId": "fe062fe6-e923-4542-910a-a9a3813cd4a7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "fIdPdPM_Uzrl"
      },
      "outputs": [],
      "source": [
        "dataset = sequences.map(split_input_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogGV3Ri_Uzrm",
        "outputId": "53193e70-8439-4a29-d367-1735eb493260"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ],
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "  print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "  print(\"Target:\", text_from_ids(target_example).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3b-r7XLUzrm"
      },
      "source": [
        "### Membuat Batch Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67aUs3ZqUzrm",
        "outputId": "9fc4ffd7-41ad-4404-b237-c626a3ffc687"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sp0Arj1HUzrm"
      },
      "source": [
        "### Buat Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "UR_cxJZPUzrm"
      },
      "outputs": [],
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "toemyZXNUzrm"
      },
      "outputs": [],
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "5AfH8MM5Uzrm"
      },
      "outputs": [],
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Bd-MilFCUzrm"
      },
      "outputs": [],
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Da-oqkfvUzrn"
      },
      "source": [
        "### Uji Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbdLPiA_Uzrn",
        "outputId": "0c103ebd-1a30-49e5-f16b-28f8998a914b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzZXAeuHUzrn",
        "outputId": "babf2238-a7ee-43f5-f9f1-4fd1c5171617"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4022850 (15.35 MB)\n",
            "Trainable params: 4022850 (15.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "fM1VoDC9Uzrn"
      },
      "outputs": [],
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0],num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaxqrAK3Uzrr",
        "outputId": "71aeabd6-e55b-40c3-f0ae-103c28d06be6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([15, 65, 44, 63,  1, 12, 15, 44, 27,  2, 50, 35, 11, 25, 32, 11, 10,\n",
              "       22, 38, 31,  0, 60, 14, 37, 65, 33, 41, 40, 48, 23, 58, 32, 11, 50,\n",
              "       15, 20, 53, 45, 53, 49, 46, 41, 15, 40, 38, 58,  4, 14, 62, 30, 11,\n",
              "       48, 65, 65, 44, 22, 33, 49, 30, 20, 47,  9, 32,  2, 64, 52, 37, 36,\n",
              "       45, 17, 19, 52, 56, 42, 25, 42, 22, 38, 38, 47, 27,  7, 51, 49, 32,\n",
              "       46, 36, 63, 31, 19, 55, 32, 51, 35,  3,  3, 11, 18,  2, 49])"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sampled_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpGRzixQUzrr",
        "outputId": "7c691d6f-e9e9-4edc-9cce-2b2189d4515e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input:\n",
            " b'rmity to mock my body;\\nTo shape my legs of an unequal size;\\nTo disproportion me in every part,\\nLike '\n",
            "\n",
            "Next Char Predictions:\n",
            " b'Bzex\\n;BeN kV:LS:3IYR[UNK]uAXzTbaiJsS:kBGnfnjgbBaYs$AwQ:izzeITjQGh.S ymXWfDFmqcLcIYYhN,ljSgWxRFpSlV!!:E j'\n"
          ]
        }
      ],
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QKg7F8SUzrr"
      },
      "source": [
        "# Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F66isR8vUzrr"
      },
      "source": [
        "### Tambahan optimizer dan fungsi loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "-LWwZWtSUzrr"
      },
      "outputs": [],
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXKhOFjzUzrr",
        "outputId": "4469ea84-ffee-4c04-8ad4-1e67f92ae527"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.189971, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Z_IK9sNUzrr",
        "outputId": "49e44108-45ee-40e1-bcf3-09629d4daa8e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "66.020874"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "gAms8ZM1Uzrr"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkSJni0aUzrr"
      },
      "source": [
        "### Konfigurasi Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "T9JjS0tyUzrs"
      },
      "outputs": [],
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b3c743AUzrs"
      },
      "source": [
        "### Lakukan Proses Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "Ay0yyQZ-Uzrs"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbsbR4fPVJDa",
        "outputId": "a7aadffa-1832-4e88-c599-082124fccae4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "172/172 [==============================] - 954s 5s/step - loss: 2.6912\n",
            "Epoch 2/10\n",
            "172/172 [==============================] - 979s 6s/step - loss: 1.9732\n",
            "Epoch 3/10\n",
            "172/172 [==============================] - 945s 5s/step - loss: 1.6987\n",
            "Epoch 4/10\n",
            "172/172 [==============================] - 973s 6s/step - loss: 1.5410\n",
            "Epoch 5/10\n",
            "172/172 [==============================] - 937s 5s/step - loss: 1.4423\n",
            "Epoch 6/10\n",
            "172/172 [==============================] - 937s 5s/step - loss: 1.3763\n",
            "Epoch 7/10\n",
            "172/172 [==============================] - 934s 5s/step - loss: 1.3240\n",
            "Epoch 8/10\n",
            "172/172 [==============================] - 941s 5s/step - loss: 1.2803\n",
            "Epoch 9/10\n",
            "172/172 [==============================] - 940s 5s/step - loss: 1.2386\n",
            "Epoch 10/10\n",
            "172/172 [==============================] - 940s 5s/step - loss: 1.1989\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPSj5tWcZQ08"
      },
      "source": [
        "### Generate Teks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "zj0mNDCrZSKq"
      },
      "outputs": [],
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "KfjUNPWQZWU8"
      },
      "outputs": [],
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrTi9DEHZc8a",
        "outputId": "d1cda846-9ca7-4b12-ac1a-0890ec76faea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROMEO:\n",
            "Subbots prince my uncle, thus here comes:\n",
            "Thy brother dead, dut on my conqueror,\n",
            "petsing to the lik, after dream't like some beast:\n",
            "O, peradventure have some cape as the deed?\n",
            "\n",
            "BENVOLIO:\n",
            "Well effeed, noble will Now is his followers!\n",
            "\n",
            "CLARENCE:\n",
            "Ay much gone, sir, as all these sorrows with\n",
            "poor Coriolanus.\n",
            "\n",
            "PRUMIO:\n",
            "With banishess sleep scrong-like such fortune passions,\n",
            "But pity Lord Servingman:\n",
            "No, now my soul mount, as toward to heal on me\n",
            "some tale, signion with our secret: his witchon\n",
            "That varued the sea? and you lion. Would he die in King\n",
            "Discourse proud coward: baheful youth,\n",
            "I have no more success in Morthoo, bath her to horse:\n",
            "Now, by my satisffert so lament yourself,\n",
            "Were it not her no capa; thy slatterers dooth,\n",
            "I shall stone from the very fault not;\n",
            "But, to-morrow or York not him so, as it had\n",
            "By one that I shall bear.\n",
            "\n",
            "CORIOLANUS:\n",
            "Knew'st soften, the state, which he there,\n",
            "Would he prosperous and dark goads,\n",
            "Of sorrow primutely of the Lady Balingbroke\n",
            "The ponds of more proud \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.510775327682495\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nlEYkGbZf0E",
        "outputId": "d575b1b1-1f6c-4e17-a57a-78ae88462536"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nI tell you, if you know brother?\\n\\nPROSPERO:\\nDost know it, come, you content,\\nBriaf and loving hour, go blyond,\\nTo stick your very shadow of France,\\nWhose hand, if we stoop to beat will put me;\\nHave not him sing fair nobleight of their heads\\nand heir is dispatch with me. Has expiscide the season\\nTo waze me froth an inch pretty claim\\nThe son of consure's fair and hand,\\nThat till her fortunate in fuil own wonds\\nBy woe, in love, rise and fiery for.\\n\\nPRANCO:\\nHere so a sleeping treacherous med nigh 'Sy,\\nGo to prison, since the inches; he would they drive.\\n\\nVOLUMNIA:\\nThey should leave him hote?\\n\\nFROTH:\\nShall not be seen, and learn'd to-morrow murseless,\\nHow doth shine arts the point's even soul's dood?\\n\\nKING RICHARD IIIN:\\nAs most is: it shall not be: in love I ben\\nOn to be present. Yourself more counsink,\\nWhich we dispitions end,\\nWhither our Horpon's Camillo woe no.\\n\\nDUKE VINCENTIO:\\nI will not fail had I never man well;\\nIf thou like customany. They would be free Angelo\\ncome absouns without m\"\n",
            " b\"ROMEO:\\nAy, if my farther now?\\n\\nSICINIUS:\\nPreat son, I promise their holy cheeks!\\n\\nISABELLA:\\nAs they do restraint me bring your tears!\\n\\nLUCENTIO:\\nIt is the right quietnes of none service,\\nOnly let them leave they are now gone\\nWhither or the main poor country earth,\\nOr thou host marriage, steed away! Sir, 'tis fareden valour\\nThe gate of youth not good:\\nSo prison extremocies doth.\\nTake then rain them in the vouch o' the canle of you;\\nBites or so retirence him hand and love.\\n\\nTRANIO:\\nMadam, and we might well for't:\\nA thousand fool, I will not be high city.\\n\\nBARNADNA:\\nI\\nAm no bone of war; no gooder distinguis me.\\n\\nKATHARINA:\\nI knew no son, who should it do him\\nconsented, not wherein he loves them, the\\nshepherds might so give him dead; where's od\\nTo no even sent be lock'd;\\nWhere 'he heavens, to apprehens him.\\n\\nLORD be\\nEre O Righa:\\nGood father, stand still, who thou beace must be\\nwhor spoils which you denied the night frail.\\n\\nKING HENRY VI:\\nFall wear him, I shows it.\\n\\nCLARENCE:\\nKing Menenius Marger\"\n",
            " b\"ROMEO:\\nHis tunning cold will it shame to prince by\\nThese gracious sperish. Good morrow, my lord,\\nYou say, 'tis gone to put it in a gain at\\nManglate cousin lives of Montagues, a task,--\\n\\nPETRUCHIO:\\nAs good done, I will seem tone, lit\\nlack thee how he siness' now well pocket your person like age,\\nThat counterpeize York upon this sige.\\n\\nKING HENRY VI:\\nSarway with me: embrain to come,\\nIn being pratery in thy valiant, wear your sway?\\nO, my gracious lord!\\n\\nKATHARINA:\\nI will making more than your horse:\\nTherefore, I fear home appeased me a country\\nTaste me amiss: which, I pray at hind;\\nAnd for our will thy worship's tongue\\nEven in compass'd: my seize dies, you must make\\nRied and more may never sears me too heldom,\\nTo her some pave Edward may know it\\nFor heart thou wert, here's couguner a pots;\\nStating, and every prize the con and rabst will speak?\\nSay now you come the ports of all the elmest.\\n\\nGREMIO:\\nI Inamis, Nather let me give.\\n\\nKATHARINA:\\nA beggar of honour, and the heese than story!\\nHow though\"\n",
            " b\"ROMEO:\\nI have heard of this converts wint,\\nThen should before him and their servinens\\nDo I will be married to see she conleys\\nAre come against the heaven souls of thee.\\n\\nKATHARIN:\\nYou must indeed it makes one discontent come?\\n\\nBINTOLIO:\\nWill you serve length it.\\n\\nMERCUTIO:\\nNow, would?\\n\\nARCHIDAMUS:\\nWell, ghos wondrou never a sailor with\\nI see of breath; the fool, which stoop Marcius\\nWe but fear'd and honour in the poully to\\npur Than cherish Bosemain home.\\n\\nCIRIOLANUS:\\nYou whom is frankly do help me victory.\\n\\nKATHARINA:\\nI' a gentle of bitter news, wring a jand my son?\\n\\nLord:\\nThou joy of Spirries: stay?\\n\\nSirst Senator:\\nNo men for for your blood,\\nTo woman in his sake, God do her eyes:\\n'Tis no man's pack or go entertain it;\\nO, with these priteous face is shamed by.\\nI would you must pity these news.\\n\\nGRUMIO:\\nCome hother, to brust her keed.\\n\\nSecond Gentleman:\\nOne was my wretched--all out of mine.\\n\\nGRUMIO:\\nHe did blush before him. So doth\\nFaults not the scent ill's, set down the unkitness,\\nAre stain\"\n",
            " b\"ROMEO:\\nGentlemen, York God,'ll mean me hear.\\nBut do it, will I not--\\n\\nSLARIOLA:\\nHad you still calm thee not, my lord, he one.\\n\\nKING RICHARD III:\\nGood my lord.\\n\\nHERMIONE:\\nBastard; sir, we pred before I sent not thinking.\\n\\nMERCUTIO:\\nA securious canopel, crown\\n'Than fitting wits a hold, all this underies\\nTo save them something.\\n\\nABHORSON:\\nSo I am? Come, can my good sibe; tho own treason,\\nBut he did ever unworthy man more told: but it\\nindeed, please you, if you live on infercoment\\nBy every officers in our heels,\\nWhen the tongue or are to her hereon of my parpos,\\nPoxts my asking-mination, they blood\\nCut on the candle of the house of Gree, With my\\nmine honey that is any his tridious but myself\\nWas famal cauces, offelledless journeys\\nTo way already to my Lord of Buckingham.\\n\\nKATHARINA:\\nAh so roundly in my son be an ensortue's.\\n\\nBUCKINGHAM:\\nYour hold-hidmed are palled, we have in death be seruly\\nGo well enough a sovereign thild,\\nTo levy no more shoes, which seldom king\\nTo lively hard him Angelo.\\n\\nGR\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 5.898772239685059\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iagV17QZklr"
      },
      "source": [
        "# Ekspor Model Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axRXMeuBZgjN",
        "outputId": "ccc3ee74-b793-45c8-c52c-a887e441f413"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7c83bc087d00>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
          ]
        }
      ],
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rFeZp-VZmNL",
        "outputId": "2af3e17e-3d9d-4e57-b987-24b91aeaaff3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROMEO:\n",
            "Yet, air come, come.\n",
            "\n",
            "RUCYORD:\n",
            "By the world, my royal princess, poor soul!\n",
            "O, yet, awaked all this \n"
          ]
        }
      ],
      "source": [
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tugas JS 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomTraining(MyModel):\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "      inputs, labels = inputs\n",
        "      with tf.GradientTape() as tape:\n",
        "          predictions = self(inputs, training=True)\n",
        "          loss = self.loss(labels, predictions)\n",
        "      grads = tape.gradient(loss, model.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "      return {'loss': loss}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = CustomTraining(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.fit(dataset, epochs=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    mean.reset_states()\n",
        "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "        logs = model.train_step([inp, target])\n",
        "        mean.update_state(logs['loss'])\n",
        "\n",
        "        if batch_n % 50 == 0:\n",
        "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "            print(template)\n",
        "\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "    print()\n",
        "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
        "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
        "    print(\"_\"*80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
