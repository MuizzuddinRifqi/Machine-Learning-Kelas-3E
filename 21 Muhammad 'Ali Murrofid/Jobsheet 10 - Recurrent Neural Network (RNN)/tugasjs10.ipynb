{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alimurrofid/Machine-Learning-Kelas-3E/blob/main/21%20Muhammad%20'Ali%20Murrofid/Jobsheet%2010%20-%20Recurrent%20Neural%20Network%20(RNN)/tugasjs10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nwvw6hyqUzrc"
      },
      "source": [
        "### Import TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ys1pI_c5Uzrg"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0Iw2YF5Uzrh"
      },
      "source": [
        "### Download Dataset Shakespeare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDI4cjqiUzri",
        "outputId": "0a2d65c2-e9c9-4029-ffdc-f4aff82523f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buQOnNW4Uzri"
      },
      "source": [
        "Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9sZ5ctAUzrj",
        "outputId": "7461a579-eb62-450c-ebcb-0033d353abbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ],
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChFNUvLLUzrj",
        "outputId": "250812cc-a295-4514-f04c-5a932a2fd610"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HZY-reXUzrj",
        "outputId": "c7b1163c-9819-4a9f-802b-5c898adf1915"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ],
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcUJRaXFUzrj"
      },
      "source": [
        "### Olah Teks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l15iwtzOUzrj"
      },
      "source": [
        "Vectorize Teks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IU1iij2CUzrj",
        "outputId": "8d474111-e2c7-430d-a133-cadcc12567b1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LzEh4gVVUzrj"
      },
      "outputs": [],
      "source": [
        "\n",
        "ids_from_chars = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCYOze7gUzrk",
        "outputId": "729b0e59-b9a5-4a37-b05f-ae4e727fa20d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "sqCxDZ0rUzrk"
      },
      "outputs": [],
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRZmW84UUzrk",
        "outputId": "552a537e-ad40-4ca6-bec7-29aa57bf3149"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0aCGGmOUzrk",
        "outputId": "4b40594f-caaa-438d-daf4-13215bfabfec"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "19mABPfEUzrk"
      },
      "outputs": [],
      "source": [
        "def text_from_ids(ids):\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zcCMbS2Uzrk"
      },
      "source": [
        "### Membuat Trianing Set dan Target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xmxvtMiUzrk",
        "outputId": "ef8846e6-4ede-4d28-ffb1-6232532ce3f9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Uglw_a8aUzrl"
      },
      "outputs": [],
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9MMpBIXUzrl",
        "outputId": "9b88d05d-930f-4187-dd60-d9242ecf212a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ],
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "jTy63gBlUzrl"
      },
      "outputs": [],
      "source": [
        "seq_length = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MY3dBm4IUzrl",
        "outputId": "2d52bb4f-5f06-4147-cc07-421ce4d94077"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9OXANDLUzrl",
        "outputId": "a0bb8268-609e-49fc-9323-8a3e69dbd3da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ],
      "source": [
        "for seq in sequences.take(5):\n",
        "    print(text_from_ids(seq).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "FoQ5A9h5Uzrl"
      },
      "outputs": [],
      "source": [
        "def split_input_target(sequence):\n",
        "  input_text = sequence[:-1]\n",
        "  target_text = sequence[1:]\n",
        "  return input_text, target_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeUCl8TeUzrl",
        "outputId": "0cb46145-b615-41e1-eb97-037a31291c13"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "fIdPdPM_Uzrl"
      },
      "outputs": [],
      "source": [
        "dataset = sequences.map(split_input_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogGV3Ri_Uzrm",
        "outputId": "5bb8bb05-d76d-4675-b28b-edeef82e780e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ],
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "  print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "  print(\"Target:\", text_from_ids(target_example).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3b-r7XLUzrm"
      },
      "source": [
        "### Membuat Batch Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67aUs3ZqUzrm",
        "outputId": "2ed2d459-31a1-43ce-e69c-bd3bfc3b1307"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sp0Arj1HUzrm"
      },
      "source": [
        "### Buat Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "UR_cxJZPUzrm"
      },
      "outputs": [],
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "toemyZXNUzrm"
      },
      "outputs": [],
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "5AfH8MM5Uzrm"
      },
      "outputs": [],
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Bd-MilFCUzrm"
      },
      "outputs": [],
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Da-oqkfvUzrn"
      },
      "source": [
        "### Uji Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbdLPiA_Uzrn",
        "outputId": "dace3847-d413-43f6-b91d-4b74d7c0a1ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzZXAeuHUzrn",
        "outputId": "5a83a72c-3e94-4c18-8f48-078c6b82b157"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4022850 (15.35 MB)\n",
            "Trainable params: 4022850 (15.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "fM1VoDC9Uzrn"
      },
      "outputs": [],
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0],num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaxqrAK3Uzrr",
        "outputId": "6ea06b58-9490-436c-d853-0919a92cbb65"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([37, 64, 18, 59, 49, 13, 13, 13, 39, 62, 28, 18, 35, 61, 30, 34, 48,\n",
              "       35, 36, 20, 51, 27, 40, 60, 25, 45, 18, 42,  5,  8,  6, 52, 33, 45,\n",
              "        2, 17, 11, 48, 35, 32, 32,  9, 15, 44,  4, 23, 53,  5, 30, 41, 25,\n",
              "       15, 34,  4, 20, 44, 13, 39, 27, 56, 31, 30, 13, 11, 37, 39, 33,  3,\n",
              "       21, 41, 27, 34, 23, 55, 46, 60, 24, 40, 52, 13, 22, 27, 38, 19, 51,\n",
              "        4, 38, 42, 10, 49, 28,  8, 51, 26, 28, 44,  6, 49,  4, 33])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "sampled_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpGRzixQUzrr",
        "outputId": "2cc30e2a-1d4d-4b15-f53a-3cb3941e3e68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b\"nt of their master's death and\\nin the view of the shepherd: so that all the\\ninstruments which aided \"\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"XyEtj???ZwOEVvQUiVWGlNauLfEc&-'mTf D:iVSS.Be$Jn&QbLBU$Ge?ZNqRQ?:XZT!HbNUJpguKam?INYFl$Yc3jO-lMOe'j$T\"\n"
          ]
        }
      ],
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QKg7F8SUzrr"
      },
      "source": [
        "# Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F66isR8vUzrr"
      },
      "source": [
        "### Tambahan optimizer dan fungsi loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "-LWwZWtSUzrr"
      },
      "outputs": [],
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXKhOFjzUzrr",
        "outputId": "fcd7aad7-f9db-48e0-ffa9-364e1048e20f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.1893544, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Z_IK9sNUzrr",
        "outputId": "58c6eec2-7bc8-48f5-eb55-2ce8bc9b599d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65.98019"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "gAms8ZM1Uzrr"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkSJni0aUzrr"
      },
      "source": [
        "### Konfigurasi Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "T9JjS0tyUzrs"
      },
      "outputs": [],
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b3c743AUzrs"
      },
      "source": [
        "### Lakukan Proses Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "Ay0yyQZ-Uzrs"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbsbR4fPVJDa",
        "outputId": "cfe28c75-b67f-4dce-f8d8-977ec3687739"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "172/172 [==============================] - 13s 53ms/step - loss: 2.7444\n",
            "Epoch 2/10\n",
            "172/172 [==============================] - 11s 53ms/step - loss: 2.0023\n",
            "Epoch 3/10\n",
            "172/172 [==============================] - 11s 52ms/step - loss: 1.7186\n",
            "Epoch 4/10\n",
            "172/172 [==============================] - 11s 52ms/step - loss: 1.5554\n",
            "Epoch 5/10\n",
            "172/172 [==============================] - 12s 53ms/step - loss: 1.4536\n",
            "Epoch 6/10\n",
            "172/172 [==============================] - 12s 54ms/step - loss: 1.3851\n",
            "Epoch 7/10\n",
            "172/172 [==============================] - 11s 53ms/step - loss: 1.3310\n",
            "Epoch 8/10\n",
            "172/172 [==============================] - 11s 53ms/step - loss: 1.2865\n",
            "Epoch 9/10\n",
            "172/172 [==============================] - 12s 54ms/step - loss: 1.2454\n",
            "Epoch 10/10\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 1.2043\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPSj5tWcZQ08"
      },
      "source": [
        "### Generate Teks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "zj0mNDCrZSKq"
      },
      "outputs": [],
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "KfjUNPWQZWU8"
      },
      "outputs": [],
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrTi9DEHZc8a",
        "outputId": "051e0bfe-a8cc-4b99-d4ac-3c6371c4fd73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "See when that his fault too.\n",
            "\n",
            "KING RICHARD III:\n",
            "Marry, noble Kate: what says he shall not slace? us kere well away!\n",
            "Unless the be, not by Angelo do not am.\n",
            "\n",
            "Nurse:\n",
            "Hold this in thy accouting fire in himself;\n",
            "Then here to break a word will ckable--thee\n",
            "Dismiss'd to scorns of such enceose. So pass, was I under thy\n",
            "dost thou, Herririon: God there, sir, tell thee\n",
            "Well make our gentle reason and the heavens three.\n",
            "\n",
            "QUEEN:\n",
            "And I have none, your companies much.\n",
            "\n",
            "GREMIO:\n",
            "Softly, a shouldy mischarge! O hearts!\n",
            "\n",
            "COMINIUS:\n",
            "You are too part:\n",
            "The duke is company.\n",
            "\n",
            "KING RICHARD III:\n",
            "And thou must fall approvest, who should he underne reign:\n",
            "Hall half, you're forth for her tyranch, my life Ancentry,\n",
            "Refent live.\n",
            "\n",
            "Second Bushy:\n",
            "For God's studd, for I dim at that sir,\n",
            "This a schate rebute with night-borner part thys,\n",
            "From a treacherous softer fortune's pleasure:\n",
            "Intend just being him slips to most soet,\n",
            "And watch'd with weather-flesh and quoth he true--Here,\n",
            "Which else he's wenstorward chosen apon my  \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.201212167739868\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nlEYkGbZf0E",
        "outputId": "d27a3a27-ec8b-4429-c6b4-bebb76606556"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nRedumb, of it: Fresh 'ema'd me double an a\\nrequeat's plegging suppect, and the beople\\nWhich to dry this, at ghost in meet;\\nAnd to prbyit shall resolve me from the sword heart.\\n\\nTYBALT:\\nPray Ghou. Was ever brought to hear? I am sore you--\\n\\nWhat dost thou till my horse Duke of York?\\n\\nVICKINGHAM:\\nYet not decaise you speak too tale about i' the clow\\nHen commend with me. The sovereign cloves her off:\\nAnd these I tend thee news.\\n\\nLADY ANNE:\\nNever believe thee Lancaster.\\n\\nQUEEN ELIZABETH:\\nWhy, young parther, my company, my sir, for the resolut come\\nTo glantage of our most deeds defended wonder.\\n\\nROMEO:\\nHad long amburged, since fetch me too hate.\\nCareet master BaRMARDINENIUS:\\nI pray thee, welcome hath he that is suck not\\nincherchanged to an envy, might prace, in the wantons,\\nThat must be rather wash'd in dost thou,\\nForswarn, restayent mine arm, batterning thise.\\n\\nGONZALO:\\nCall thee, Clarence shall we held repair, Dray we hear from me,\\nThen wear as we do injudan'd: brought will perfume,\\nThese \"\n",
            " b\"ROMEO:\\nOrt Romeo, Richmond thou bitterness: my liege,\\nYou hopes a sin of heaven, changed come.\\nWhat's the writing.\\n\\nQUEEN:\\nShe say the lord of heavy. Or bid me,\\nWhat's the measure that we met? his misure all else!\\nWhat is' awake? do you year is the most heart?\\n\\nKATHARINA:\\nNo, my lord.\\n\\nFirst Senator:\\nNobleBland, the kease comes that wonder here rather to you would ha?\\nAnrock thee in the house, I'll give us free myself,\\nWe two husband fect of our mans adversain:\\nWhat's trifusers you in the very days the would half is grace,\\nAnd with his hand would be left nothing but smeal tongue\\nFaredereth is her brets for the minister,\\nThe heavens for my ternivy should view the\\nwing; who take me in his hand assistance\\nwill hold to have a fallied offices?\\n\\nANGELO:\\nYou renden haste, madam: I'ld take my lands any changear.\\n\\nDUKE VINCENTIO:\\nIt he repORF\\nI was boil and wife; for if shield you well,\\nHis lamentain and deep desire to my stats,\\nSick mady Angelo, resealth him up and wonder,\\nTo thy fairer ditins from \"\n",
            " b\"ROMEO:\\nSport it is as our name told you:\\nThen, this, soon made o'erbrest,\\nThe shappery further was three-black with\\nThe sea, whose would I must tell you talk of\\nherce, I call thee but not treason.\\n\\nVERCIUT:\\nAnd Bolingbrong but office! my knows;\\nGod kings Suchold's reason, knock morewect on mine earth\\nAnd brings my body withds the lawful tod!\\nYou knew my dear, and some begn fail!\\nFourly craves the speech in except of more.\\nThy virtues of loveth he usurply else\\nHer two breast to so sleep. Here's dead!\\n\\nJULIET:\\nAy, by me, and, what! say\\nThat's hateful ancient great Apollo's deed?\\n\\nGEORGLA:\\nWhat, wouldst talker all humbling?\\n\\nFirst Servingman:\\nWho made peace?\\n\\nBRAKENBURY:\\nI seem too hate him mistress, fierch you all\\nand myself the hear of his meliman woes,\\nAll him of honour beague, that enchantest of this cheating command,\\nI love as this well as consent as some already;\\nselfusted thines.\\n\\nCOMINIUS:\\nThen dream'd not\\nComen prison. If thus much other sevelance,\\nshe dissever from the place o' the em\"\n",
            " b\"ROMEO:\\nThen 'tis delivered. Edwas do I, we will follow!\\n\\nNurse:\\nMy Lady Baptista!\\n\\nQUEEN:\\n\\nMARCIUS:\\nReleased him, here's never, mine obedience.\\nWell, My Exemong and robbards but a case.\\nPervake thee for my blessing duke.\\n\\nKING HENTIO VIRCENTIO:\\nThere's as can let me was but folliars blessim\\nThat happy fly. Did you see this framely\\nDone so truld the sight.\\n\\nHORTENSIO:\\nMayar, the queen?\\nSoon my oath must be patience.\\nI shall hap to hear it? How need I saw!\\n\\nPOMPEY:\\nThe twraiger war,\\nSo should he are so his; Amen!\\nCall me where he shall not rather maid his ease:\\nFor what I love myself?\\n\\nALOBES:\\nCome: mischance, Lord obed Alacch us, Dope, sir. God save you, race before.\\nYou must know her father's flights, all things of men\\nI' the deed was hander for annot.\\nWell, Herein Smallair less here in respected: my turn\\nAre at my dames the see how to instrument his deat?\\n\\nLEONTES:\\nAnd he so base yet for long raised\\nFor every parted be fleece to bettee it.\\n\\nISABELLA:\\nI must\\nnee-sizeners.\\n\\nKATHARINA:\\nTake hi\"\n",
            " b\"ROMEO:\\nLet us he't, a better last castom of your brows\\nChildren he and Clarence' dlubs, any heart\\nAs he was burst unto the hay let me retire.\\n\\nCORIOLANUS:\\nYou have proud upon thy charter, knows,\\nNot money's mates from her rewards;\\nAnd thou mistress' knees have jest of your yet.\\n\\nPETRUCHIO:\\nNay, coming will thou tell them, what says young, thou canst.\\nO, then,\\n'Tis but the lord of Claudio about,\\nNor furble than gilly she's a sentenance will be in our\\nhead. No fearful name of heaven, good friend.\\nFirst, for suchood is 'sectafes agate\\nThat thou hast the boldness friar.\\nGive me your majesty; I would saw him we may show\\nthough this wretch.' Edward, here at those goodly.\\n\\nGLOUCESTER:\\nThere's no need a boot of her.\\n\\nVOLUMNIA:\\nAnd, teath, and would I was too trive:\\nThink you, for this Capulet mark?\\nAnd, lot it so my wrong'd, 'scaped from hair, all revenged\\nOf golden prisoner, then In every haste\\nThe dulght be glad to the duke was nuire Tybalt.\\nWell, fellow, speak it, if my news.\\n\\nCLARENCE:\\nRethonked\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 4.309988975524902\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iagV17QZklr"
      },
      "source": [
        "# Ekspor Model Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axRXMeuBZgjN",
        "outputId": "a26b9f0c-d539-49c8-a8b7-7e9572cd6cae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x78f81c763fd0>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
          ]
        }
      ],
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rFeZp-VZmNL",
        "outputId": "fe315594-60f9-400d-be26-8960f06728c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "So pluck in this eye all goment of his hand\n",
            "Than itself a half an evil Anow,\n",
            "And rost affairs propp\n"
          ]
        }
      ],
      "source": [
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7q9xpg5x83wl"
      },
      "source": [
        "# Tugas JS 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "DUHM3JOd83wl"
      },
      "outputs": [],
      "source": [
        "class CustomTraining(MyModel):\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "      inputs, labels = inputs\n",
        "      with tf.GradientTape() as tape:\n",
        "          predictions = self(inputs, training=True)\n",
        "          loss = self.loss(labels, predictions)\n",
        "      grads = tape.gradient(loss, model.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "      return {'loss': loss}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "UohNsjhF83wl"
      },
      "outputs": [],
      "source": [
        "model = CustomTraining(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "HF2JGrTE83wl"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "uSzzMb8C83wl",
        "outputId": "f5ef01c6-2d46-4975-c7fd-b31db523a9d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "172/172 [==============================] - 14s 55ms/step - loss: 2.7544\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x78f80921ace0>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "model.fit(dataset, epochs=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "4NagtjL783wl",
        "outputId": "faf74b39-f4de-4d4a-fc70-4b59101204d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 2.1971\n",
            "Epoch 1 Batch 50 Loss 2.1045\n",
            "Epoch 1 Batch 100 Loss 1.9864\n",
            "Epoch 1 Batch 150 Loss 1.8814\n",
            "\n",
            "Epoch 1 Loss: 2.0160\n",
            "Time taken for 1 epoch 12.12 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 2 Batch 0 Loss 1.8701\n",
            "Epoch 2 Batch 50 Loss 1.7734\n",
            "Epoch 2 Batch 100 Loss 1.7159\n",
            "Epoch 2 Batch 150 Loss 1.7021\n",
            "\n",
            "Epoch 2 Loss: 1.7412\n",
            "Time taken for 1 epoch 10.52 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 3 Batch 0 Loss 1.6578\n",
            "Epoch 3 Batch 50 Loss 1.6019\n",
            "Epoch 3 Batch 100 Loss 1.5200\n",
            "Epoch 3 Batch 150 Loss 1.5124\n",
            "\n",
            "Epoch 3 Loss: 1.5770\n",
            "Time taken for 1 epoch 11.79 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 4 Batch 0 Loss 1.4850\n",
            "Epoch 4 Batch 50 Loss 1.4738\n",
            "Epoch 4 Batch 100 Loss 1.4458\n",
            "Epoch 4 Batch 150 Loss 1.4628\n",
            "\n",
            "Epoch 4 Loss: 1.4740\n",
            "Time taken for 1 epoch 11.34 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 5 Batch 0 Loss 1.4123\n",
            "Epoch 5 Batch 50 Loss 1.4016\n",
            "Epoch 5 Batch 100 Loss 1.4136\n",
            "Epoch 5 Batch 150 Loss 1.4154\n",
            "\n",
            "Epoch 5 Loss: 1.4021\n",
            "Time taken for 1 epoch 10.75 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 6 Batch 0 Loss 1.3361\n",
            "Epoch 6 Batch 50 Loss 1.3048\n",
            "Epoch 6 Batch 100 Loss 1.2984\n",
            "Epoch 6 Batch 150 Loss 1.3421\n",
            "\n",
            "Epoch 6 Loss: 1.3478\n",
            "Time taken for 1 epoch 10.84 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 7 Batch 0 Loss 1.2846\n",
            "Epoch 7 Batch 50 Loss 1.3015\n",
            "Epoch 7 Batch 100 Loss 1.3613\n",
            "Epoch 7 Batch 150 Loss 1.3123\n",
            "\n",
            "Epoch 7 Loss: 1.3020\n",
            "Time taken for 1 epoch 11.16 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 8 Batch 0 Loss 1.2423\n",
            "Epoch 8 Batch 50 Loss 1.2278\n",
            "Epoch 8 Batch 100 Loss 1.2648\n",
            "Epoch 8 Batch 150 Loss 1.2466\n",
            "\n",
            "Epoch 8 Loss: 1.2616\n",
            "Time taken for 1 epoch 11.67 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 9 Batch 0 Loss 1.1962\n",
            "Epoch 9 Batch 50 Loss 1.2451\n",
            "Epoch 9 Batch 100 Loss 1.2350\n",
            "Epoch 9 Batch 150 Loss 1.2674\n",
            "\n",
            "Epoch 9 Loss: 1.2228\n",
            "Time taken for 1 epoch 11.93 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 10 Batch 0 Loss 1.1642\n",
            "Epoch 10 Batch 50 Loss 1.1922\n",
            "Epoch 10 Batch 100 Loss 1.1779\n",
            "Epoch 10 Batch 150 Loss 1.2393\n",
            "\n",
            "Epoch 10 Loss: 1.1850\n",
            "Time taken for 1 epoch 11.02 sec\n",
            "________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    mean.reset_states()\n",
        "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "        logs = model.train_step([inp, target])\n",
        "        mean.update_state(logs['loss'])\n",
        "\n",
        "        if batch_n % 50 == 0:\n",
        "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "            print(template)\n",
        "\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "    print()\n",
        "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
        "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
        "    print(\"_\"*80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "orig_nbformat": 4,
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}