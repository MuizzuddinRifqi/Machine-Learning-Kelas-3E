{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Praktikum 2 - Generator Teks dengan RNN**"
      ],
      "metadata": {
        "id": "OlYPlFS25-a5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Setup**"
      ],
      "metadata": {
        "id": "IOfyDSg96J2j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import TensorFlow**"
      ],
      "metadata": {
        "id": "8ydAWYuN6N8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "metadata": {
        "id": "XeA9MvOq6hUc"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Download Dataset Shakespeare**"
      ],
      "metadata": {
        "id": "086H1e1_6pcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9yuz4AI6qR9",
        "outputId": "d2bb6078-ce9a-401b-fc10-d2582db2439a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Data**"
      ],
      "metadata": {
        "id": "Ol3f_qJm61qD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpI2R9CQ63X_",
        "outputId": "39a918b1-a0b6-4e9a-dc5c-92e584cb33cd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNMEnpLP68bs",
        "outputId": "db0fe6bd-2cb1-4730-f1e2-78f7123fafe0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9EXXRqr6-qV",
        "outputId": "c468591c-6f2b-4ea2-9ffb-f30bf53ffbb1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Text Processing**"
      ],
      "metadata": {
        "id": "l8TXxdi97Fec"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Vectorize**"
      ],
      "metadata": {
        "id": "GbKQskxs7PNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_texts = ['abcdefg', 'xyz'] # converts a string to a numeric representation, the text will be split into tokens first\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mDXqw6a7HMk",
        "outputId": "0dd21d58-194c-479c-c550-a7ff28c8631c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup( # create tf.keras.layers.StringLookup layer\n",
        "vocabulary=list(vocab), mask_token=None)"
      ],
      "metadata": {
        "id": "jCbkSB227hVF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = ids_from_chars(chars) # converting token to id\n",
        "ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mP3u2OMw8EGi",
        "outputId": "062ce18e-a60d-4a96-c8a5-4abd983e5048"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup( # inverts this representation using tf.keras.layers.StringLookup\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "metadata": {
        "id": "9jZZVp3o8S1M"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = chars_from_ids(ids) # converts back the characters from the ID vector, and returns them as tf.RaggedTensor characters\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wakkQGtY8mTh",
        "outputId": "193a8070-a532-4fa0-8dd2-fe3c62e66d32"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy() # use tf.strings.reduce_join to rejoin characters into a string"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWPW6X6h8ylW",
        "outputId": "3ba014ad-fa1f-4d06-b719-7f03301e78d2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "YbRCK9wU87kR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Prediction**"
      ],
      "metadata": {
        "id": "oTV7S-xP9EKP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create Training Sets and Targets**"
      ],
      "metadata": {
        "id": "35DIzlWa9Jyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8')) # divide text into sequence examples\n",
        "all_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DssuI1989GpM",
        "outputId": "3c0d225f-e651-4bea-b0a7-63c1d4eef61f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "eafu25Oh9cSg"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdkFuPrB9fxe",
        "outputId": "c54a95a0-ff9f-4fa4-ecb3-e75c9f1eee2f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100"
      ],
      "metadata": {
        "id": "a1eiDKgW9kmU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True) # using batch method to converts these individual characters to the desired size sequence.\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aq5CRc609qIF",
        "outputId": "29df08ea-718d-461e-e0a4-bfc44f491f51"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for seq in sequences.take(5): # combines the tokens back into a string\n",
        "    print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_cy5oQw971h",
        "outputId": "94e8c8ce-1f17-4713-e2ba-5bda6fbcf8ce"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence): # takes a sequence as input, duplicates it, and shifts it to align the input and labels for each time step\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "metadata": {
        "id": "ufGQpHoL-HKT"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5A6Kpgt6-U4r",
        "outputId": "30572aeb-0016-4bb4-b763-c9e3159aff25"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "mqXQEKf9-YSt"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qY-KMNLl-bzl",
        "outputId": "be99dbc8-c615-4da4-893e-3c521fae6c8c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create Training Batches**"
      ],
      "metadata": {
        "id": "Rvn2-ZU3-l6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gM1hZ6nr-pS7",
        "outputId": "8f78c076-0b41-42d3-f888-164587d115c9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Create Model**"
      ],
      "metadata": {
        "id": "5hKdA_93-tcm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "02wyJU5A-zQz"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model): # defines the model as a subclass ofkeras.Model\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "UHIfGZxr-4FD"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "kjrAdWUL-8Ph"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model Test**"
      ],
      "metadata": {
        "id": "H4z7HGcU_TvP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1): # check the shape of the output\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2FEfC84_aHp",
        "outputId": "78d358a6-6607-497d-a282-d2d5799960a6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary() # summarize the model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6hK1td-_kD9",
        "outputId": "f8001f5e-f1bc-40e4-ecc5-ffafee3c826c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4022850 (15.35 MB)\n",
            "Trainable params: 4022850 (15.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1) # try for the first example in the batch\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "metadata": {
        "id": "nWwIGyuo_rf5"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices # next character index prediction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIBAPUOK_8A-",
        "outputId": "77ee6cf9-ab48-483c-9179-5465b0a7e7b1"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([31, 65, 11, 63, 22, 31, 23, 21, 31, 57, 59, 37, 65,  3, 34, 35, 31,\n",
              "       54, 65, 13, 60, 36,  1, 57, 44, 56, 58, 53,  2, 11, 20,  2, 58,  9,\n",
              "       23,  2, 35, 35, 61, 58, 63, 38, 48, 42, 45,  4, 11, 56, 56, 18,  9,\n",
              "        6, 62,  2, 17,  9,  2, 16, 35,  3, 28, 32, 32, 52, 10, 48, 42, 45,\n",
              "       15,  8, 20, 30, 51, 11, 56,  0, 65,  7, 65, 35, 62, 64, 46, 64, 63,\n",
              "       63, 65, 60, 48, 21, 53, 35, 23,  9, 62, 33, 35, 22, 23,  8])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy()) # see the text predicted by this untrained model\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qu3VS-PqAA5D",
        "outputId": "5334ba3b-2668-4c2e-bc37-40ceef698e5b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b\"Bohemia\\nYou take my lord, I'll give him my commission\\nTo let him there a month behind the gest\\nPrefi\"\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"Rz:xIRJHRrtXz!UVRoz?uW\\nreqsn :G s.J VVvsxYicf$:qqE.'w D. CV!OSSm3icfB-GQl:q[UNK]z,zVwygyxxzuiHnVJ.wTVIJ-\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Train Model**"
      ],
      "metadata": {
        "id": "JFv000_mAMJb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Additional Optimizer and Loss Function**"
      ],
      "metadata": {
        "id": "rV3cEO-uAXna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "N-cWQZJlAVG8"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvUiJFRfAfMW",
        "outputId": "5e3999a1-c8e9-4b21-88f3-5c8be3e681f6"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.1887736, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.exp(example_batch_mean_loss).numpy() # checks that the exponential of the average loss should be approximately equal to the vocabulary size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WO7-vRsAj1v",
        "outputId": "ff7af303-aabd-4e22-9d49-17ec832503ba"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65.94187"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss) # configure the training procedure using the tf.keras.Model.compile method"
      ],
      "metadata": {
        "id": "QaGKyNfhAl1y"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Configure Checkpoints**"
      ],
      "metadata": {
        "id": "JwQ3hwFmA7Oh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "nuyphwT5A-Lf"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Carry out the Training Process**"
      ],
      "metadata": {
        "id": "OeUUC11EBIeX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20"
      ],
      "metadata": {
        "id": "LdD3QGAFBLDl"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0yu73jwBSw5",
        "outputId": "71a766d7-97b1-4dfa-ebbe-25de1cf1691a"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 16s 62ms/step - loss: 2.7115\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 1.9847\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 13s 59ms/step - loss: 1.7102\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.5496\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.4502\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 13s 61ms/step - loss: 1.3816\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 13s 62ms/step - loss: 1.3294\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 13s 58ms/step - loss: 1.2846\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 13s 57ms/step - loss: 1.2436\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 12s 57ms/step - loss: 1.2036\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 11s 58ms/step - loss: 1.1653\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 1.1243\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.0816\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 13s 60ms/step - loss: 1.0355\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.9887\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.9379\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 13s 60ms/step - loss: 0.8864\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 13s 59ms/step - loss: 0.8350\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 0.7836\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.7336\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Text Generate**"
      ],
      "metadata": {
        "id": "3vu25KkCCvHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "aED0IpdPC1o_"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "kAKc2rdgC4lM"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_8mk4MXC_qm",
        "outputId": "5b31e8c8-d458-4cd4-e581-cc009225ab85"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "The foul wings affords not her then small counsel,\n",
            "To satisfy your form and noblehes.\n",
            "\n",
            "CLEOMENES:\n",
            "Sir, the best father's son, anon of you;\n",
            "For thou wilt-wicked action lends him hence!\n",
            "Roweless brave better by the heart, to show yourselves;\n",
            "Laments as one deigh. Right you him my friends be too resire\n",
            "Stands again; because I raiment and\n",
            "The place of your mother in the encounter.\n",
            "\n",
            "SLY:\n",
            "Mark'd your highness prove a long such pporising flass,\n",
            "From forth thou thoughts to the grace, where I must not,\n",
            "For down as I can: dune my nay, but doubtful\n",
            "Business in thy death, nor he did look on;\n",
            "And let your Romeo will be cold.\n",
            "\n",
            "GREMIO:\n",
            "Why, there too! thou hast calls' a brother\n",
            "And that he did; as he hath baught a tide\n",
            "To fly, if I die, for all the western sleep,\n",
            "Hooding your conference, which never was it,\n",
            "Or not a joint might speak--\n",
            "Much hope it is.\n",
            "\n",
            "First Citizen:\n",
            "Our dear son, sir; for this was the farther that\n",
            "He shall not.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "So, O, thou citizens, but he shall stay\n",
            "not quickly,  \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.4676625728607178\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time() # the model produces 5 outputs in almost the same time as it took to produce 1 output previously\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIcbOLGWDJJG",
        "outputId": "e3e6c2b2-e163-43c4-8c7c-6230bb2df43a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nWell, all festerdanning, of these knights. Worthy help it,\\nYo, she lays honesty.\\n\\nGRUMIO:\\nWhy, then less heartish braw out of your got.\\nThus, I throw my face, my Romeo is im.\\n\\nGLOUCESTER:\\n\\nCLARENCE:\\n\\nBold:\\nWelcome to my bossol.\\n\\nBRUTUS:\\nIs the wondrous beat-bed, thus' I believe thee,\\nWhen I was born in Henry's nest,\\nOr weeding bones and what a thing\\nThat word, respective likewise hath embraced,\\nAnd safely hither to use me from his city,\\nAnd he shull dissemble with free and grown wombs\\nTo see our good companion. So, now I remain,\\nHave stoop themselves; and therein fair sustier trush\\nAnd never beating for this aftering, and I'll venve\\nyou so seem'd, away; for once again to this sword;\\nIt calls not Kate; behold him first, thou thereby\\nfaced that which you in a marbly take them hope.\\nHere is the case put in this sweet books,\\nNor so severe flaunts, as to the entertainms.\\nWe are lecius.\\n\\nSICINIUS:\\nGo before, be it leave:\\nHow doth the country, whereaf these stranges he die?\\n\\nJULIET:\\nWhat sai\"\n",
            " b\"ROMEO:\\nO bloody, time shall I reneen away?\\n\\nDUKE OF YORK:\\nMy lords, for your villain in this news!--\\nWarwick, what affroncest thou recontime hath,\\nWhen opposed heaven she should be my hope,\\nMight help homelight thus: but if he do continue\\nThou causes of unco right. Come, go with me:\\nClerd you our prierongs of the glory beal in herself.\\nThe deed, despecture much of that shrinking with\\nthis party and father, dishonour'd upper, their\\ntrue opposed against a breath;\\nHad pleasuden he shries, the mountain I should not;\\nAway, awake the prince my brother does,\\nStabb'd by Someopities and obts, being memas\\nShall be her friendly tear the blood as by\\npinished. Fear and noble Clearur man actorness!\\nOr shall never walk or dead!\\n\\nRICHMOND:\\nNo, madam; yet leisure use us nature.\\n\\nSecond Murderer:\\nNor noble men; he shall be denied by for grief:\\nTo be so honourable fit for his shapes,\\nAnvice and decombed to see a store;\\nAnd I, that standing unfold die again;\\nThe king his rest, that he should succers,\\nYou have a\"\n",
            " b\"ROMEO:\\nGhost; as\\nseek he hath great aside the furthest cheque yield;\\nOne or honest with the crown, and thou\\nart for: the statue is but learn, if he'll frame to death,\\nI may not bear thee hence; what you did\\nThy frages become of himself, hearting you\\nOf what you gave her? The roy at once\\nrequest your grace to be access\\nDestroy'd it and too late for him.\\n\\nFirst Senator:\\nNeither.\\n\\nDORCAS:\\nFollow me.\\n\\nLADY ANNE:\\nBefore with him, our bid herselves hid them.\\n\\nRICHARD:\\nNorfell'd! O monstrous steel, for this night's good\\nPlessea hath set-doughter to him. And thou this meeting\\nWas he given, and that which, entreat you\\nThat ever action Aumerle is dead.\\n\\nDUCHESS OF YORK:\\nGlack-night: tell me what they deserve!\\n\\nISABELLA:\\nThou art not in, let us suffer when my dee should\\nbed.\\n\\nApatet:\\nCome, come, we knows not, honourable: he's again:\\nI pray, he's a love; and beating men to keep\\nThe noble turns of all my fortunes aids,\\nThat make me speak it, vent your goods,\\nforswear these minits now to see him: I\\nloved \"\n",
            " b\"ROMEO:\\nSir, so, now but well; thou seest with patience\\nWill that you will but look upon thee.\\n\\nClown:\\nNow a white Barnardine upon Some comes.\\nBut what say you?\\n\\nARGEL:\\nWe then, we will, my lord.\\n\\nKING EDWARD IV:\\nIs prack and daughter Katharina like a drunk,\\nWe should have staw'd the child.\\n\\nLUCIO:\\nDespair'st my liege,\\nThrough I dream with Pempremetes are wretch\\nWithout a dull extedual times, nor wait unto these wars! What, my boy,\\nShall be a county mothers for the field,\\nAnd they but cannot counterfail thee in the sea\\nWherein your ladial seat convert to bite\\nWhere no man here at Febbling throat, it seams\\nIs ghattlen makes him war: when he wakes about a\\ncounted loss and persuts desire yourself returned,\\nProcure your mother, that thou hast spoke to\\ndie with me. Let one of those whose,\\nmore cover'd: the leisure I cheque mine or honours shall\\nThe keys be out.' 'Fore near; the more is spring;\\nThe other like perform'd.\\n\\nCOMINIUS:\\nMarcius your dute?\\n\\nESCALUS:\\nAy, but not denied; I know him not, sir\"\n",
            " b\"ROMEO:\\nWell, then well ready sweets?\\n\\nISABELLA:\\nThe eak quarrel since I say, a wish too.\\n\\nHORTENSIO:\\nTake our fortune is, and to deck our shamp,\\nThe other to Buckingham? Nay, come again;\\nOur wonds and neath and victory;\\nOne greed in jest, only, that you have spoke,\\nWith murder so emptrey words. But 'tis a gammand;\\nThen be who told, that I have beheld thy walfily\\nWere for itself a saint-sweed, worth arms show\\nThat mandles rice, forsworn to gue be sound with eye,\\nAnd from this letter, then.\\n\\nISABELLA:\\nI know she is mighty, one actor.\\n\\nJULIET:\\nGood! I would there was not nor favour.\\n\\nDUKE OF YORK:\\nSpeak from those that kill move no falsehood of my banishment:\\nFrom monature for a milling king by whence!\\nThou liest, and night, all o'er, and kneel his face.\\nMinethanch more strike, for then yet groans,\\nTo see him an infort to saint-since,\\nShe's victory.\\nBut come, thou wouldst take awrace here;\\nIt is too good, but better'd to ruin of you:\\nWhat wear it, since?\\nthink you of it! our dukedom there to op\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.4579410552978516\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Export Generator Model**"
      ],
      "metadata": {
        "id": "9Dd1vCAVDVw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step') # saves the model one step, allowing to use it wherever tf.saved_model is received\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kG1LHNrDcgI",
        "outputId": "ee0a1ff1-2588-4115-a486-a07042509f9e"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x78111fff5360>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8WsOlOVDqaj",
        "outputId": "faa13480-f9e1-4409-e1dc-339c6279c5b3"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "This being sport my breaking that's mictry, but I\n",
            "would over leave to visit me ups!\n",
            "\n",
            "WARWICK:\n",
            "Neith\n"
          ]
        }
      ]
    }
  ]
}