{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Praktikum 1 - Klasifikasi üê± dan üê∂**"
      ],
      "metadata": {
        "id": "18fqPu4DW0t-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 1 - Import Library**"
      ],
      "metadata": {
        "id": "JV0LNTHBW5gh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# access and manipulate files stored in your Google Drive directly from the Colab notebook\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HChfZ-dWxx8",
        "outputId": "76442c20-3834-4a88-abe9-932f749ed13e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# imports two essential libraries, tensorflow and ImageDataGenerator for working with deep learning and image data preprocessing\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "kEf42P3OYQ_1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 2 - Data Pre Processing**"
      ],
      "metadata": {
        "id": "260mZQmWYY2W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step 2.1 - Pre Processing Data Training**"
      ],
      "metadata": {
        "id": "yiCF7zd0YflY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defines an ImageDataGenerator object named train_datagen for augmenting and normalizing training images\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "\n",
        "# utilizes the flow_from_directory() method of the ImageDataGenerator object train_datagen to generate batches of training data\n",
        "training_set = train_datagen.flow_from_directory('/content/drive/MyDrive/dataset/training_set',\n",
        "                                                 target_size = (64, 64),\n",
        "                                                 batch_size = 32,\n",
        "                                                 class_mode = 'binary')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qv_NRv7WYe7e",
        "outputId": "93964d66-8def-4b49-9711-d786a037bd7f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 8000 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step 2.2 - Pre-Processing Testing Data**"
      ],
      "metadata": {
        "id": "AzG5Y2nstM4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defines an ImageDataGenerator object named test_datagen for normalizing test images\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "# utilizes the flow_from_directory() method of the ImageDataGenerator object test_datagen to generate batches of test data\n",
        "test_set = test_datagen.flow_from_directory('/content/drive/MyDrive/dataset/test_set',\n",
        "                                            target_size = (64, 64),\n",
        "                                            batch_size = 32,\n",
        "                                            class_mode = 'binary')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkeLC_YktVpJ",
        "outputId": "077d0689-8e1b-4760-f560-b426caebee3e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2000 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 3 - CNN Model Building**"
      ],
      "metadata": {
        "id": "bAW_0CLzuCEm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step 3.1 - CNN Model Initiation**"
      ],
      "metadata": {
        "id": "VhR6bmOpuI5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initializes a sequential model object named cnn using TensorFlow Keras\n",
        "cnn = tf.keras.models.Sequential()"
      ],
      "metadata": {
        "id": "xTZSYUJjuLS7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step 3.2 - Creation of Convolution Layer 1**"
      ],
      "metadata": {
        "id": "IrvO4aBmuaOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# adds a convolutional layer to the sequential model cnn\n",
        "# this convolutional layer will extract features from the input images, such as edges, textures, and color patterns\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))"
      ],
      "metadata": {
        "id": "q5CDqbj8udjQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step 3.3 - Creation of Pooling Layer 1**"
      ],
      "metadata": {
        "id": "dvv3Sdbku7mg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# adds a max pooling layer to the sequential model cnn\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
      ],
      "metadata": {
        "id": "UW1D0aVBvBA2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step 3.4 - Creation of Convolution Layer 2 and Pooling 2**"
      ],
      "metadata": {
        "id": "Ur570CP6vkmb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# adds a second convolutional layer to the sequential model cnn\n",
        "# this second convolutional layer will further extract features from the input feature maps, refining the patterns detected by the first convolutional layer\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
        "\n",
        "# adds another max pooling layer to the sequential model cnn\n",
        "# by adding another max pooling layer, the model further reduces the dimensionality of the feature maps, making them more manageable for the subsequent layers\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
      ],
      "metadata": {
        "id": "y2qq_-Ynvn4T"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step 3.5 - Flattening**"
      ],
      "metadata": {
        "id": "mMgpn4d7wHEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# adds a flatten layer to the sequential model cnn\n",
        "cnn.add(tf.keras.layers.Flatten())"
      ],
      "metadata": {
        "id": "e2bT5sl4wJ-m"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step 3.6 - Fully Connected Layer 1 (Input)**"
      ],
      "metadata": {
        "id": "1vu8ilFmwuH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# adds a fully connected (dense) layer to the sequential model cnn\n",
        "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))"
      ],
      "metadata": {
        "id": "egO13mK5wz6a"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step 3.7 - Fully Connected Layer 2 (Output)**"
      ],
      "metadata": {
        "id": "GXo0Ux-UyGG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# adds the final fully connected layer to the sequential model cnn\n",
        "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "HNF-2ZQwyI5G"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step 3.8 - Compile Model CNN**"
      ],
      "metadata": {
        "id": "44phPHntyUis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# configures the training process for the convolutional neural network model cnn\n",
        "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "bB6obLkyyUHd"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 4 - Fit CNN**"
      ],
      "metadata": {
        "id": "sL6eDkFtyj7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# trains the convolutional neural network model cnn using the prepared training and validation data sets\n",
        "cnn.fit(x = training_set, validation_data = test_set, epochs = 25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QF7aDnchyoYm",
        "outputId": "1076d7e6-6e53-4035-8c55-0419ce8aa3d9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "250/250 [==============================] - 1850s 7s/step - loss: 0.6830 - accuracy: 0.5544 - val_loss: 0.6478 - val_accuracy: 0.6375\n",
            "Epoch 2/25\n",
            "250/250 [==============================] - 36s 145ms/step - loss: 0.6412 - accuracy: 0.6460 - val_loss: 0.6055 - val_accuracy: 0.6820\n",
            "Epoch 3/25\n",
            "250/250 [==============================] - 36s 144ms/step - loss: 0.6046 - accuracy: 0.6718 - val_loss: 0.5730 - val_accuracy: 0.7150\n",
            "Epoch 4/25\n",
            "250/250 [==============================] - 36s 144ms/step - loss: 0.5715 - accuracy: 0.7038 - val_loss: 0.5762 - val_accuracy: 0.6980\n",
            "Epoch 5/25\n",
            "250/250 [==============================] - 42s 166ms/step - loss: 0.5395 - accuracy: 0.7308 - val_loss: 0.5083 - val_accuracy: 0.7445\n",
            "Epoch 6/25\n",
            "250/250 [==============================] - 36s 146ms/step - loss: 0.5178 - accuracy: 0.7430 - val_loss: 0.5051 - val_accuracy: 0.7520\n",
            "Epoch 7/25\n",
            "250/250 [==============================] - 37s 146ms/step - loss: 0.4969 - accuracy: 0.7571 - val_loss: 0.5148 - val_accuracy: 0.7465\n",
            "Epoch 8/25\n",
            "250/250 [==============================] - 36s 143ms/step - loss: 0.4714 - accuracy: 0.7696 - val_loss: 0.5035 - val_accuracy: 0.7635\n",
            "Epoch 9/25\n",
            "250/250 [==============================] - 35s 142ms/step - loss: 0.4539 - accuracy: 0.7847 - val_loss: 0.4703 - val_accuracy: 0.7785\n",
            "Epoch 10/25\n",
            "250/250 [==============================] - 36s 144ms/step - loss: 0.4457 - accuracy: 0.7855 - val_loss: 0.4801 - val_accuracy: 0.7725\n",
            "Epoch 11/25\n",
            "250/250 [==============================] - 36s 145ms/step - loss: 0.4213 - accuracy: 0.8029 - val_loss: 0.4668 - val_accuracy: 0.7765\n",
            "Epoch 12/25\n",
            "250/250 [==============================] - 36s 143ms/step - loss: 0.4067 - accuracy: 0.8111 - val_loss: 0.4841 - val_accuracy: 0.7830\n",
            "Epoch 13/25\n",
            "250/250 [==============================] - 36s 145ms/step - loss: 0.3932 - accuracy: 0.8184 - val_loss: 0.4578 - val_accuracy: 0.7880\n",
            "Epoch 14/25\n",
            "250/250 [==============================] - 36s 143ms/step - loss: 0.3665 - accuracy: 0.8328 - val_loss: 0.4682 - val_accuracy: 0.7845\n",
            "Epoch 15/25\n",
            "250/250 [==============================] - 36s 145ms/step - loss: 0.3523 - accuracy: 0.8438 - val_loss: 0.4843 - val_accuracy: 0.7855\n",
            "Epoch 16/25\n",
            "250/250 [==============================] - 35s 142ms/step - loss: 0.3411 - accuracy: 0.8468 - val_loss: 0.4628 - val_accuracy: 0.7900\n",
            "Epoch 17/25\n",
            "250/250 [==============================] - 36s 142ms/step - loss: 0.3248 - accuracy: 0.8591 - val_loss: 0.5011 - val_accuracy: 0.7845\n",
            "Epoch 18/25\n",
            "250/250 [==============================] - 36s 145ms/step - loss: 0.3087 - accuracy: 0.8676 - val_loss: 0.4632 - val_accuracy: 0.7965\n",
            "Epoch 19/25\n",
            "250/250 [==============================] - 36s 144ms/step - loss: 0.2872 - accuracy: 0.8786 - val_loss: 0.4689 - val_accuracy: 0.7980\n",
            "Epoch 20/25\n",
            "250/250 [==============================] - 36s 143ms/step - loss: 0.2803 - accuracy: 0.8830 - val_loss: 0.5162 - val_accuracy: 0.7910\n",
            "Epoch 21/25\n",
            "250/250 [==============================] - 36s 145ms/step - loss: 0.2604 - accuracy: 0.8919 - val_loss: 0.5579 - val_accuracy: 0.7895\n",
            "Epoch 22/25\n",
            "250/250 [==============================] - 36s 142ms/step - loss: 0.2532 - accuracy: 0.8966 - val_loss: 0.5685 - val_accuracy: 0.7750\n",
            "Epoch 23/25\n",
            "250/250 [==============================] - 35s 142ms/step - loss: 0.2325 - accuracy: 0.9021 - val_loss: 0.5315 - val_accuracy: 0.8050\n",
            "Epoch 24/25\n",
            "250/250 [==============================] - 36s 145ms/step - loss: 0.2176 - accuracy: 0.9135 - val_loss: 0.5340 - val_accuracy: 0.7995\n",
            "Epoch 25/25\n",
            "250/250 [==============================] - 40s 162ms/step - loss: 0.2187 - accuracy: 0.9126 - val_loss: 0.5773 - val_accuracy: 0.7940\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e58783d52a0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 5 - Prediction with 1 Image**"
      ],
      "metadata": {
        "id": "M30MwkRi7ZGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing libraries\n",
        "import numpy as np\n",
        "\n",
        "# loading image\n",
        "from keras.preprocessing import image\n",
        "test_image = image.load_img('/content/drive/MyDrive/dataset/single_prediction/cat_or_dog_1.jpg', target_size = (64, 64))\n",
        "\n",
        "# converting image to array and expanding dimension\n",
        "test_image = image.img_to_array(test_image)\n",
        "test_image = np.expand_dims(test_image, axis = 0)\n",
        "\n",
        "# feeds the preprocessed image array into the trained CNN model cnn and obtains the prediction\n",
        "result = cnn.predict(test_image)\n",
        "\n",
        "# interprets the prediction to determine whether the image is a cat or a dog\n",
        "training_set.class_indices\n",
        "if result[0][0] == 1:\n",
        "  prediction = 'dog'\n",
        "else:\n",
        "  prediction = 'cat'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTC3Yl7J7ifJ",
        "outputId": "06b8c354-e44a-4d5c-e8d1-7880e37e97ae"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 203ms/step\n"
          ]
        }
      ]
    }
  ]
}