{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**TUGAS**"
      ],
      "metadata": {
        "id": "QT6hwxPW9rGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "metadata": {
        "id": "e-ZovydOTfk7"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "\n"
      ],
      "metadata": {
        "id": "k_mJr_ALbMuu"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2NQJazobPJn",
        "outputId": "52afb55f-b2f7-4209-fb97-0af28093feda"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wP2XHTZ7bRcq",
        "outputId": "0187e794-6b9e-4128-d4c9-2ad0475eeac3"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YnWeNKDbTmR",
        "outputId": "d94c9cd7-8ecc-4b86-a6bf-43d27ed6e163"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print unique characters\n",
        "for char in vocab:\n",
        "    print(char, end=' ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBbyXlxJbhI0",
        "outputId": "04816ee8-6f5a-4aca-80a2-deb21075f00f"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "   ! $ & ' , - . 3 : ; ? A B C D E F G H I J K L M N O P Q R S T U V W X Y Z a b c d e f g h i j k l m n o p q r s t u v w x y z "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZCQdVghbi9H",
        "outputId": "b9b344b3-7315-4c1a-be55-a72c399de866"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None)"
      ],
      "metadata": {
        "id": "wpq4CwiabqEU"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CJKHs49bswj",
        "outputId": "81a47f14-ca53-49e8-cda4-75a14245292c"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
        ""
      ],
      "metadata": {
        "id": "89qRDb2dbvHh"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OC192GbobxDT",
        "outputId": "c515f171-0a6e-4eeb-cc3d-f3cc94fcbc5f"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ld3U1visbyg1",
        "outputId": "ffd8b865-fd20-4dc7-fc73-a19ba27a4448"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "3qJ7RzMob06S"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2q7i4fLcACr",
        "outputId": "e7ac6a02-d06c-4fa8-c33a-6a9587b1cc59"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "sdY_OsDhcC_5"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdd7mOvNcEhA",
        "outputId": "82d52a01-f25f-452d-bacc-20674d2b9344"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100\n"
      ],
      "metadata": {
        "id": "hvG2zMk2cG-l"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Metode batch dengan mudah mengonversi karakter individual ini menjadi urutan ukuran yang diinginkan.\n",
        "\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZAiCiUAcJeL",
        "outputId": "0c56d95d-6c85-4201-d563-b20ef19e2ee0"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for seq in sequences.take(5):\n",
        "    print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCd5OWP8cUzB",
        "outputId": "d919482f-83db-4b07-93e5-fefa737596da"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "  input_text = sequence[:-1]\n",
        "  target_text = sequence[1:]\n",
        "  return input_text, target_text"
      ],
      "metadata": {
        "id": "vFd6exiHcbj3"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKtVp-G9cL1U",
        "outputId": "51182169-52b9-4727-877b-dcc798bb1f55"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "q36i5yJicf7D"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "  print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "  print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juSpRiv0cjPw",
        "outputId": "fc2839ec-2eae-4ec6-8fe2-9c5a3de85376"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kX8fkCrScmPc",
        "outputId": "5b1fcdb7-8915-4add-9bf4-6421b1413956"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "dxyMdj1Wctjf"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "XZ6gkTFCcwqT"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "XnnhTPSoc0Hr"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#uji model\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5P9BGaMcc1yK",
        "outputId": "44aff368-6b13-497b-d435-91bf33e9cb63"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvzDD9wRc7WA",
        "outputId": "5c1e62aa-6bcb-480b-93ab-76d98e9563ff"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     multiple                  16896     \n",
            "                                                                 \n",
            " gru_1 (GRU)                 multiple                  3938304   \n",
            "                                                                 \n",
            " dense_1 (Dense)             multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4022850 (15.35 MB)\n",
            "Trainable params: 4022850 (15.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0],num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "metadata": {
        "id": "r224fBxJc-pn"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4N1cEn5dBHi",
        "outputId": "6ee7846a-fe6d-44ee-adf2-2ee0865746fe"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 9,  5, 51, 15, 62, 37, 48, 59, 40,  8, 39,  7, 50,  1, 24,  2, 34,\n",
              "       26, 37, 40,  8, 11, 19, 14, 44, 32,  4, 24, 49, 43, 14,  2, 39, 23,\n",
              "       37, 61, 19, 24,  1, 17, 65, 43, 10, 19,  8, 26, 26, 54, 61,  1, 33,\n",
              "       26,  1,  9, 65,  8, 49,  1, 32, 52,  9, 55, 46, 44,  3, 29, 21, 59,\n",
              "        0, 63, 37, 27, 51, 20,  4, 14, 15, 61, 36,  3, 51, 29, 17, 45, 24,\n",
              "       62, 42, 26, 18,  2, 44, 11, 16,  4, 59, 62, 42, 54, 63, 33])"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9CaO2TFdDDy",
        "outputId": "ec22f45c-3993-4afa-abd6-1add74c603c2"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b\"r are your eyebrows?\\n\\nFirst Lady:\\nBlue, my lord.\\n\\nMAMILLIUS:\\nNay, that's a mock: I have seen a lady'\"\n",
            "\n",
            "Next Char Predictions:\n",
            " b'.&lBwXita-Z,k\\nK UMXa-:FAeS$KjdA ZJXvFK\\nDzd3F-MMov\\nTM\\n.z-j\\nSm.pge!PHt[UNK]xXNlG$ABvW!lPDfKwcME e:C$twcoxT'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "6NhQmmEwdGqb"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFDxXFkjdJrg",
        "outputId": "830191cb-b998-4e22-d321-3c8b86ab6633"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.190182, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Yj0MdO3dLdx",
        "outputId": "38ed515b-498e-4f0f-b16f-beb71e07dbd5"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66.03482"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "zyfTU03BdN99"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "WXU92hPJdQxJ"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10"
      ],
      "metadata": {
        "id": "FnjOeanhdSUo"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpJwFtw-dU1T",
        "outputId": "79faf21c-7d37-4c1e-98c2-4030e28ba4ff"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "172/172 [==============================] - 15s 57ms/step - loss: 2.7182\n",
            "Epoch 2/10\n",
            "172/172 [==============================] - 16s 64ms/step - loss: 1.9884\n",
            "Epoch 3/10\n",
            "172/172 [==============================] - 14s 57ms/step - loss: 1.7132\n",
            "Epoch 4/10\n",
            "172/172 [==============================] - 12s 55ms/step - loss: 1.5513\n",
            "Epoch 5/10\n",
            "172/172 [==============================] - 13s 57ms/step - loss: 1.4522\n",
            "Epoch 6/10\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.3834\n",
            "Epoch 7/10\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.3302\n",
            "Epoch 8/10\n",
            "172/172 [==============================] - 12s 56ms/step - loss: 1.2848\n",
            "Epoch 9/10\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.2431\n",
            "Epoch 10/10\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.2039\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# membuat prediksi satu langkah:\n",
        "\n",
        "\n",
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "TMZ7IF1BdW8t"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "7Ne1q-czj3n3"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BW6uEkmjj_MZ",
        "outputId": "63942cc6-602f-4462-8864-2e25703f7259"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "Adieu, good stay, pur, heaven, yields; out against me.\n",
            "\n",
            "LEONTES:\n",
            "Now, sir, if not his\n",
            "undergrance!\n",
            "Must be pureful: our fiest and falsehood\n",
            "God know thee and a black is for the gent.\n",
            "\n",
            "PAULINA:\n",
            "Why, I am crame\n",
            "For beauty and selest upon your heom\n",
            "of such a reash.\n",
            "\n",
            "CAMILLO:\n",
            "Alack, be it son, but for thou namest\n",
            "You. in for me, help! it shall see we will;\n",
            "Who in the pall hour mighty straight at once\n",
            "a kind? she'll brain, Lewis camebully.\n",
            "\n",
            "MENENIUS:\n",
            "Why, touch choses suitoward out the other,\n",
            "Whose hopel would slew thou one than Do ask\n",
            "Our condxict of abuses: they could cannot speak\n",
            "Well well may keep you affaits from him on simples, and a cloin-\n",
            "\n",
            "GONZALO:\n",
            "Wherefor I, in good lord,\n",
            "But now it are for menine\n",
            "That your presentuptier conterpets died too\n",
            "Creward, look'd and roint me for her own,\n",
            "And so you consent in my brother\n",
            "You that knew makes Buleasing pritceen plain,\n",
            "For 'twould I'll prove thee all, that ne'er dismistues,\n",
            "Whose souls o'er last Auftley's life at that deight you.\n",
            "\n",
            "QUEEN MA \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 4.81290602684021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yg5pJ7uqkBc3",
        "outputId": "ca3dd372-5055-475d-fcfe-18d4fe0f5ad5"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nGive me your table, combary.\\n\\nBAPOT:\\nGood friends than spood up almost soul.\\n\\nPAMIRL:\\nMy lord,\\nMy husband lies thee tears.\\n\\nJOHN OF GAUNT:\\nNo, by 'I am speak.\\n\\nThore:\\nOut, I that same you.\\n\\nLUCIO:\\n\\nHORTENSIO:\\n\\nROMEO:\\nMy heart with showshreps to you speak;\\nAnd thou wilt did usurp on us.\\n\\nSecond Gentleman:\\nI will be counted's year;\\nFor I will bring me or truth more reason\\nWhen I swear that thou condution will keep thee\\nSlugh on to be duty fit the king his spain-forw'd\\nAnd sen in thy kinsman.'\\n\\nGLOMAS:\\nJoy, callet! I will ent\\nTo supple my led tcan nature down.\\n\\nBENVOLIO:\\nSuch a lematious peace:\\nMy woothy valave, and, widow, as to\\nWere but up, such distor miserable.\\n\\nBRUTUS:\\nAnd if not marry to accussems\\nFor ne'er last thoushnow to conquer while!\\n\\nGLOUCESTER:\\nBe you, for ortague, we will up.\\n\\nHERMIONE:\\nCome, come, I bedow you then,\\nHis acting with this Fortules unshape,\\nAnd bellats as hand and happy and woo'd.\\n\\nFirst Servingman:\\nAy, thy saw her run about the hand,\\nThough any ambuss doth w\"\n",
            " b\"ROMEO:\\nO, I am, nor told me the other's fault I spay,\\nTyrrels frow one seams; if he had biterves me\\ndo good tyrant upon her surace-judge,\\nGreat lightness in glories: pethile Henry'd and\\na tate of us, but 'twas a party stoil.\\nWhat stops the quaurty Dear personably,\\nYour simiss of heaven summer fellow'd frails\\nIn consented with a lover-breathed of the king.\\n\\nDUCHESS OF YORK:\\nWhat will still tell.\\n\\nFirst Servingman:\\nWho is it true? come to come to bey.\\n\\nJULIET:\\nI spirk you, too long is too but well-by,\\nHer manried with her lips he knows wear am.\\n\\nCOMINIUS:\\nThey may be so by so bride and leader?\\n\\nMIRANDA:\\nI have:\\nGoth all upon me and gile call her grow,\\nBagn point to me.\\n\\nFirst Murderer:\\nWhy what would will trust him and virw-canibeland gall\\nThat knows his flight till shed to death\\nwith pure lighthing prive and think; or temperance\\nAt it stood from my fellow you\\na prefire of an knee beats in his own life\\nSit us, a little grave shall subbial quench;\\nFor nothing hard one pain and fear\\nsome governm\"\n",
            " b\"ROMEO:\\ntrust you, my ill, I anGo content.\\n\\nLADY CAPULET:\\nWhere should be thou near fours; and touch a friend haste himself,\\nPlease you, if you traitor lay how to be a blood was he\\nA one and devenved, take acceptsible bark\\nBut odds about the precious Moneate.\\n\\nPRINCE:\\nDorget, gone and full of more chiefes.\\nLead their hearts were hilled, advannators\\nMest woe my dukes for Rome, thou crowns,\\nAnd let his lost unaxatual friend,\\nThat may be end as 'twould be curst iefus;\\nYour prison encreased with you! sir,\\nTo wine aumbroked to cliff clouds enough\\nOf me, is tere alms, but strain'd\\nTheir callets herein to a tuntage, if all\\nMisparked here perhaps and request thee a whit.\\nForgiveness but scaped those nurse, awake\\nTo often soft and a codant whipe,\\nOur approtected a little suit;\\nOur Richard's request; and I am grieved a\\nFou-carrellage up and I honey. Would tell you\\nport-provlations are more\\nthings he drawn with by any priso\\nAgainst the one afterwards: yea, may.\\nI will not see't the Duke of Clarence:\\nAnd\"\n",
            " b\"ROMEO:\\nShe as his moubh, fool worthy thrones; some sufficer at\\nFor our person whips of what-exbramed!\\nThe clotses of the time was long in it.\\n\\nDUKE VINCENTIO:\\nIf you can being a till murder'd with a Capulets;\\nShould prope it would not make his holour mine,\\nHer whole shall doth me from all butched\\nAnd stay: four art mothorous, yea,--\\n\\nMENENIUS:\\nO,\\nThat it most note of that I may go,\\nBut bett what of his name and not home?\\n\\nRATCLIFF:\\nFaith, sir, But only hearing to instruct my brother,\\nHere im me standing from his pirther murded mee.\\n\\nQUEEN MARGARER:\\nThis night's in laby,--what, we butchear the vanity:\\nWhither now instructed to be a king.\\n\\nKING EDWARD IV:\\nBut drep at homes night upon his spite.\\n\\nJULIET:\\nWhat, even now your queen too, go, after:\\nBy bounty tuner as my point.\\nCome, but two wit and traction to this.\\nWhat is a longing speech; but you rough quick\\nUpon their true, seed hath most pacuses;\\nMy flack shapes your beauty friends.\\nWhether he chance thou dost not look upon,\\nAnd that they hav\"\n",
            " b\"ROMEO:\\nI'll in your hands: I would I accourt again.\\n\\nBUCKINGHAM:\\nWhy look you, lies, I to access govern: because me stirg,\\nit is a poor fellow: then must full look on.\\n\\nProvost:\\nHere is the highman of this night thrust find\\nMy prays all eate, was stone all disgraced:\\nWith that mad have no upen in his lanities.\\nSoBe confess, our abounce, that let's well in the beat\\nMusic in the pain, sir, wiser cuttoes in me.\\n\\nGREGORY:\\nYea, or wise me!--\\nI piently joy to some grace I carms:\\nThe composife will not pity us as thou hast a daughter: then,\\nWhile bear this name? true, it is my suffer; thou\\nbloody wishing mean summer along,\\nShouldiem us have more well bawdaying,\\nSoftend, as I am you'll be your faull's turn?\\n\\nKING RICHARD III:\\nThe presently-good quarrel hath with those thou art?\\n\\nISABELLA:\\nLet's temporious and--\\n\\nMERCUTIO:\\nO my trunk nither?\\n\\nMENENIUS:\\nThou lay's deal away; for I have an extremity\\nI seek some hand, urgungled, that care for that condemned\\nAnd galled an enemy.\\n\\nCAMILLO:\\nCome he, or whe\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 4.1871497631073\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xef7wOnNkHPH",
        "outputId": "58f5d8c1-6887-493c-84b7-2415024d11c1"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7e8375fcae30>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]"
      ],
      "metadata": {
        "id": "IIg6eiYfkNel"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWyqhzwZkR9t",
        "outputId": "116ccbb6-0cad-4cc6-e56a-28c4a1c03206"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "Go, cross, the law may out op his wife,\n",
            "What a youd love tell her and till now\n",
            "Me that would till h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTraining(MyModel):\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "    inputs, labels = inputs\n",
        "    with tf.GradientTape() as tape:\n",
        "      predictions = self(inputs, training=True)\n",
        "      loss = self.loss(labels, predictions)\n",
        "      grads = tape.gradient(loss, model.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "      return {'loss': loss}"
      ],
      "metadata": {
        "id": "Dn__WR79kVBD"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomTraining(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "CrUNZI3FkXrJ"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n"
      ],
      "metadata": {
        "id": "ocgSvJRmkZ-R"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(dataset, epochs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oB2HX7Vvkcj5",
        "outputId": "1eb48b95-fc7c-4f6b-e4e5-34287e7861d2"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "172/172 [==============================] - 16s 55ms/step - loss: 2.7341\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e8361ead900>"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  mean.reset_states()\n",
        "  for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "    logs = model.train_step([inp, target])\n",
        "    mean.update_state(logs['loss'])\n",
        "\n",
        "    if batch_n % 50 == 0:\n",
        "      template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "      print(template)\n",
        "\n",
        "  # saving (checkpoint) the model every 5 epochs\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "  print()\n",
        "  print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
        "  print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
        "  print(\"_\"*80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLo2Q3KzkiNo",
        "outputId": "a8e8df61-7079-4945-9f72-92926726daf5"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 2.1796\n",
            "Epoch 1 Batch 50 Loss 2.0371\n",
            "Epoch 1 Batch 100 Loss 1.9622\n",
            "Epoch 1 Batch 150 Loss 1.8575\n",
            "\n",
            "Epoch 1 Loss: 2.0017\n",
            "Time taken for 1 epoch 14.63 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 2 Batch 0 Loss 1.8731\n",
            "Epoch 2 Batch 50 Loss 1.7665\n",
            "Epoch 2 Batch 100 Loss 1.6976\n",
            "Epoch 2 Batch 150 Loss 1.6623\n",
            "\n",
            "Epoch 2 Loss: 1.7242\n",
            "Time taken for 1 epoch 20.47 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 3 Batch 0 Loss 1.5951\n",
            "Epoch 3 Batch 50 Loss 1.5771\n",
            "Epoch 3 Batch 100 Loss 1.5097\n",
            "Epoch 3 Batch 150 Loss 1.5145\n",
            "\n",
            "Epoch 3 Loss: 1.5628\n",
            "Time taken for 1 epoch 12.42 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 4 Batch 0 Loss 1.4607\n",
            "Epoch 4 Batch 50 Loss 1.4585\n",
            "Epoch 4 Batch 100 Loss 1.4824\n",
            "Epoch 4 Batch 150 Loss 1.4782\n",
            "\n",
            "Epoch 4 Loss: 1.4617\n",
            "Time taken for 1 epoch 20.47 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 5 Batch 0 Loss 1.4176\n",
            "Epoch 5 Batch 50 Loss 1.3791\n",
            "Epoch 5 Batch 100 Loss 1.3762\n",
            "Epoch 5 Batch 150 Loss 1.3874\n",
            "\n",
            "Epoch 5 Loss: 1.3930\n",
            "Time taken for 1 epoch 10.82 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 6 Batch 0 Loss 1.3061\n",
            "Epoch 6 Batch 50 Loss 1.3387\n",
            "Epoch 6 Batch 100 Loss 1.3309\n",
            "Epoch 6 Batch 150 Loss 1.3392\n",
            "\n",
            "Epoch 6 Loss: 1.3400\n",
            "Time taken for 1 epoch 20.47 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 7 Batch 0 Loss 1.3015\n",
            "Epoch 7 Batch 50 Loss 1.3141\n",
            "Epoch 7 Batch 100 Loss 1.2545\n",
            "Epoch 7 Batch 150 Loss 1.3112\n",
            "\n",
            "Epoch 7 Loss: 1.2945\n",
            "Time taken for 1 epoch 10.81 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 8 Batch 0 Loss 1.2598\n",
            "Epoch 8 Batch 50 Loss 1.2410\n",
            "Epoch 8 Batch 100 Loss 1.2344\n",
            "Epoch 8 Batch 150 Loss 1.2348\n",
            "\n",
            "Epoch 8 Loss: 1.2549\n",
            "Time taken for 1 epoch 10.86 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 9 Batch 0 Loss 1.1824\n",
            "Epoch 9 Batch 50 Loss 1.2273\n",
            "Epoch 9 Batch 100 Loss 1.2567\n",
            "Epoch 9 Batch 150 Loss 1.2324\n",
            "\n",
            "Epoch 9 Loss: 1.2153\n",
            "Time taken for 1 epoch 10.81 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 10 Batch 0 Loss 1.1565\n",
            "Epoch 10 Batch 50 Loss 1.1880\n",
            "Epoch 10 Batch 100 Loss 1.1831\n",
            "Epoch 10 Batch 150 Loss 1.1649\n",
            "\n",
            "Epoch 10 Loss: 1.1766\n",
            "Time taken for 1 epoch 11.65 sec\n",
            "________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Soal**\n",
        "\n",
        "Jalankan kode diatas dan sebutkan perbedaanya dengan praktikum 2?\n",
        "\n",
        "**Jawab**\n",
        "Pada praktikum 2, pendekatan pelatihan yang digunakan lebih sederhana dan umum, yaitu dengan model.fit. Pada tugas, pendekatan pelatihan yang digunakan lebih spesifik dan kompleks, dengan kustomisasi tertentu. Dalam pendekatan ini, metode train_step didefinisikan dalam model turunan untuk mengatur pelatihan pada tingkat batch. Secara eksplisit dilakukan perhitungan loss, gradien, dan penerapan pembaruan bobot model dengan apply_gradients. Objek tf.metrics.Mean juga digunakan untuk menghitung rata-rata loss selama pelatihan. Pendekatan ini memberikan lebih banyak kontrol dan fleksibilitas dalam pengaturan pelatihan model."
      ],
      "metadata": {
        "id": "KcawUjstks7y"
      }
    }
  ]
}